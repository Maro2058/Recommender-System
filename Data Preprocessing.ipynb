{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing relevant libraries",
   "id": "e70b5566f671947"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:17.382488Z",
     "start_time": "2025-05-24T03:49:17.368522Z"
    }
   },
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "fe131ba937477bb4",
   "metadata": {},
   "source": [
    "1. Get your kaggle.json file:\n",
    "- Go to https://www.kaggle.com/account where account is ur account name\n",
    "- Scroll to the API section\n",
    "- Click Create New API Token\n",
    "- This downloads kaggle.json to your computer (usually in Downloads folder)\n",
    "\n",
    "\n",
    "2. Put kaggle.json in the right folder\n",
    "- Move the file to this folder:\n",
    "    C:\\Users\\Morad Elshorbagy\\\\.kaggle\\\n",
    "\n",
    "If the .kaggle folder doesnâ€™t exist, create it manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "43f182751c243340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:17.413823Z",
     "start_time": "2025-05-24T03:49:17.382488Z"
    }
   },
   "source": [
    "# === STEP 0: Setup Kaggle API and download dataset ZIP ===\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "dataset_zip = 'the-movies-dataset.zip'\n",
    "output_folder = 'data'\n",
    "\n",
    "# Download the dataset ZIP only if not already downloaded\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"Downloading dataset ZIP...\")\n",
    "    api.dataset_download_files('rounakbanik/the-movies-dataset', path='.', unzip=False)\n",
    "else:\n",
    "    print(\"Dataset ZIP already downloaded.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ZIP already downloaded.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting relevant files\n",
    "we will only use ratings and movies, they contain everything we need in order to make a collaborative filtering recommender"
   ],
   "id": "cf39a18ab86903d8"
  },
  {
   "cell_type": "code",
   "id": "74c72c26997cc600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:17.476975Z",
     "start_time": "2025-05-24T03:49:17.461193Z"
    }
   },
   "source": [
    "# Create data folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === STEP 1: Extract all necessary files (only if missing) ===\n",
    "needed_files = ['ratings.csv', 'movies_metadata.csv']\n",
    "existing_files = os.listdir(output_folder)\n",
    "\n",
    "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "    for file in needed_files:\n",
    "        if file not in existing_files:\n",
    "            print(f\"Extracting {file}...\")\n",
    "            zip_ref.extract(file, path=output_folder)\n",
    "        else:\n",
    "            print(f\"{file} already extracted.\")\n",
    "\n",
    "print(\"Extraction complete.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings.csv already extracted.\n",
      "movies_metadata.csv already extracted.\n",
      "Extraction complete.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading datasets",
   "id": "c903560c22f315b8"
  },
  {
   "cell_type": "code",
   "id": "314407a2338f259e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:24.016883Z",
     "start_time": "2025-05-24T03:49:17.524219Z"
    }
   },
   "source": [
    "# === STEP 2: Load datasets ===\n",
    "print(\"Loading datasets...\")\n",
    "movies = pd.read_csv(os.path.join(output_folder, 'movies_metadata.csv'), low_memory=False)\n",
    "ratings = pd.read_csv(os.path.join(output_folder, 'ratings.csv'))\n",
    "print(\"Datasets loaded.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "a052862b82802fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:24.127940Z",
     "start_time": "2025-05-24T03:49:24.114164Z"
    }
   },
   "source": [
    "# Inspect structure of all datasets\n",
    "print(\"movies_metadata.csv columns:\\n\", movies.columns)\n",
    "print(\"ratings.csv columns:\\n\", ratings.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_metadata.csv columns:\n",
      " Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
      "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
      "       'popularity', 'poster_path', 'production_companies',\n",
      "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
      "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n",
      "ratings.csv columns:\n",
      " Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "59d66d7a635919b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:24.462332Z",
     "start_time": "2025-05-24T03:49:24.187327Z"
    }
   },
   "source": [
    "# === STEP 3: Make copies to keep originals intact ===\n",
    "movies_clean = movies.copy()\n",
    "ratings_clean = ratings.copy()\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "c1b6600924974de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:24.525051Z",
     "start_time": "2025-05-24T03:49:24.509418Z"
    }
   },
   "source": [
    "# === STEP 4: Define helper functions ===\n",
    "def parse_json_column(json_str):\n",
    "    \"\"\"Parse JSON-like string to list of names.\"\"\"\n",
    "    try:\n",
    "        items = ast.literal_eval(json_str)\n",
    "        return [item['name'] for item in items]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def safe_int_conversion(val):\n",
    "    \"\"\"Safely convert to int, return None if fails.\"\"\"\n",
    "    try:\n",
    "        return int(val)\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cleaning the data\n",
    "1. Filling NaN with 0s\n",
    "2. Removing rows with missing info\n",
    "2. Removing duplicates\n",
    "3. Removing irrelevant columns like timestamp\n",
    "4. Ensuring all columns are of the right type"
   ],
   "id": "c19021d014088327"
  },
  {
   "cell_type": "code",
   "id": "8e83d628dd36897f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:31.139698Z",
     "start_time": "2025-05-24T03:49:24.572179Z"
    }
   },
   "source": [
    "# === STEP 5: Clean movies metadata ===\n",
    "print(\"Cleaning movies metadata...\")\n",
    "# Convert budget and revenue to numeric, fill NaN with 0\n",
    "movies_clean['budget'] = pd.to_numeric(movies_clean['budget'], errors='coerce').fillna(0)\n",
    "movies_clean['revenue'] = pd.to_numeric(movies_clean['revenue'], errors='coerce').fillna(0)\n",
    "\n",
    "# Drop rows missing critical info\n",
    "movies_clean = movies_clean.dropna(subset=['title', 'id'])\n",
    "\n",
    "# Convert 'id' to numeric and drop invalid rows\n",
    "movies_clean['id'] = pd.to_numeric(movies_clean['id'], errors='coerce')\n",
    "movies_clean = movies_clean.dropna(subset=['id'])\n",
    "movies_clean['id'] = movies_clean['id'].astype(int)\n",
    "\n",
    "# Parse genres column (JSON string) into list of genre names\n",
    "movies_clean['genres'] = movies_clean['genres'].apply(parse_json_column)\n",
    "# Replace empty genres lists with ['Unknown']\n",
    "movies_clean['genres'] = movies_clean['genres'].apply(lambda x: x if x else ['Unknown'])\n",
    "print(\"Movies metadata cleaned.\\n\")\n",
    "\n",
    "# === STEP 6: Clean ratings data ===\n",
    "print(\"Cleaning ratings data...\")\n",
    "# Drop duplicates (same userId, movieId)\n",
    "ratings_clean = ratings_clean.drop_duplicates(subset=['userId', 'movieId'])\n",
    "# Drop rows with missing essential columns\n",
    "ratings_clean = ratings_clean.dropna(subset=['userId', 'movieId', 'rating'])\n",
    "# Convert types properly\n",
    "ratings_clean['userId'] = ratings_clean['userId'].astype(int)\n",
    "ratings_clean['movieId'] = ratings_clean['movieId'].astype(int)\n",
    "ratings_clean['rating'] = ratings_clean['rating'].astype(float)\n",
    "ratings_clean = ratings_clean.drop('timestamp', axis=1)\n",
    "\n",
    "print(\"Ratings data cleaned.\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning movies metadata...\n",
      "Movies metadata cleaned.\n",
      "\n",
      "Cleaning ratings data...\n",
      "Ratings data cleaned.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "ca894aa0d612f851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:31.203228Z",
     "start_time": "2025-05-24T03:49:31.187173Z"
    }
   },
   "source": [
    "# === STEP 10: Summary info ===\n",
    "print(f\"Movies dataset shape: {movies_clean.shape}\")\n",
    "print(f\"Ratings dataset shape: {ratings_clean.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies dataset shape: (45460, 24)\n",
      "Ratings dataset shape: (26024289, 3)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the files",
   "id": "f7516d33c82b4c86"
  },
  {
   "cell_type": "code",
   "id": "80c0583baf3f9d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:49:51.962430Z",
     "start_time": "2025-05-24T03:49:31.251201Z"
    }
   },
   "source": [
    "# === STEP 11: Save cleaned data to CSV for reuse ===\n",
    "movies_clean.to_csv(os.path.join(output_folder, 'movies_full_clean.csv'), index=False)\n",
    "ratings_clean.to_csv(os.path.join(output_folder, 'ratings_clean.csv'), index=False)\n",
    "\n",
    "print(f\"Cleaned datasets saved in '{output_folder}' folder.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned datasets saved in 'data' folder.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extracting correct posters url for frontend\n",
    "We will use the TMDb API to get the correct poster URLs for each movie.\n",
    "\n",
    "heavily recommend using kaggle or colab for their GPUs\n"
   ],
   "id": "850045ddb5ee1449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T06:19:15.327016Z",
     "start_time": "2025-05-24T06:19:14.752120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your movie CSV\n",
    "df = pd.read_csv(os.path.join(output_folder, 'movies_full_clean.csv'))\n",
    "df = df[df['id'].notnull()].head(5000)\n",
    "df['id'] = df['id'].astype(int)\n",
    "df = df.drop(['adult', 'belongs_to_collection','budget','genres','homepage', 'imdb_id',\t'original_language', 'original_title',\t'overview',\t'popularity', 'production_companies',\t'production_countries',\t'release_date',\t'revenue',\t'runtime',\t'spoken_languages',\t'status',\t'tagline',\t'video', 'vote_average',\t'vote_count'], axis=1)\n",
    "\n",
    "\n",
    "df[\"movieId\"] = df.index + 1\n",
    "\n",
    "api_key = \"ddcd46c520d289bef23dc0f9a303a79c\"\n",
    "base_url = \"https://api.themoviedb.org/3/movie/{}?api_key={}\"\n",
    "poster_base_url = \"https://image.tmdb.org/t/p/w500\"\n",
    "\n",
    "# If partial file exists, resume\n",
    "output_file = os.path.join(output_folder, 'movies_with_tmdb_data.csv')\n",
    "if os.path.exists(output_file):\n",
    "    df_saved = pd.read_csv(output_file)\n",
    "\n",
    "    # Make sure columns exist before update\n",
    "    if 'poster_url' not in df.columns:\n",
    "        df[\"poster_url\"] = \"\"\n",
    "    if 'fetched_title' not in df.columns:\n",
    "        df[\"fetched_title\"] = \"\"\n",
    "\n",
    "    start_index = len(df_saved)\n",
    "    df.update(df_saved)\n",
    "    print(f\"Resuming from index {start_index}...\")\n",
    "else:\n",
    "    df[\"poster_url\"] = \"\"\n",
    "    df[\"fetched_title\"] = \"\"\n",
    "    start_index = 0\n",
    "\n",
    "# Loop through TMDb IDs\n",
    "for i in range(start_index, len(df)):\n",
    "    row = df.iloc[i]\n",
    "    tmdb_id = row[\"id\"]\n",
    "    try:\n",
    "        url = base_url.format(int(tmdb_id), api_key)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            df.at[i, \"poster_url\"] = (\n",
    "                poster_base_url + data.get(\"poster_path\", \"\") if data.get(\"poster_path\") else \"\"\n",
    "            )\n",
    "            df.at[i, \"fetched_title\"] = data.get(\"title\", \"\")\n",
    "        else:\n",
    "            print(f\"Error for ID {tmdb_id}: {response.status_code}\")\n",
    "        if i % 200 == 0:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Checkpoint saved at row {i}\")\n",
    "        time.sleep(0.25)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {tmdb_id}: {e}\")\n",
    "\n",
    "df = df.rename(columns={\"id\": \"tmdbId\"})\n",
    "# Final save\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"Done. Enriched metadata saved to:\", output_file)\n"
   ],
   "id": "5d46eaace505c92b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from index 5000...\n",
      "Done. Enriched metadata saved to: data\\movies_with_tmdb_data.csv\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
