{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing relevant libraries",
   "id": "e70b5566f671947"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T15:29:26.618604Z",
     "start_time": "2025-05-25T15:29:26.587113Z"
    }
   },
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import time\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "fe131ba937477bb4",
   "metadata": {},
   "source": [
    "1. Get your kaggle.json file:\n",
    "- Go to https://www.kaggle.com/account where account is ur account name\n",
    "- Scroll to the API section\n",
    "- Click Create New API Token\n",
    "- This downloads kaggle.json to your computer (usually in Downloads folder)\n",
    "\n",
    "\n",
    "2. Put kaggle.json in the right folder\n",
    "- Move the file to this folder:\n",
    "    C:\\Users\\Morad Elshorbagy\\\\.kaggle\\\n",
    "\n",
    "If the .kaggle folder doesnâ€™t exist, create it manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "43f182751c243340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:03:26.866999Z",
     "start_time": "2025-05-25T14:02:31.897476Z"
    }
   },
   "source": [
    "# === STEP 0: Setup Kaggle API and download dataset ZIP ===\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "dataset_zip = 'movielens-1m-dataset.zip'\n",
    "output_folder = 'data'\n",
    "\n",
    "# Download the dataset ZIP only if not already downloaded\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"Downloading dataset ZIP...\")\n",
    "    api.dataset_download_files('odedgolden/movielens-1m-dataset', path='.', unzip=False)\n",
    "else:\n",
    "    print(\"Dataset ZIP already downloaded.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset ZIP...\n",
      "Dataset URL: https://www.kaggle.com/datasets/odedgolden/movielens-1m-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading dataset ZIP...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m     api\u001B[38;5;241m.\u001B[39mdataset_download_files(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modedgolden/movielens-1m-dataset\u001B[39m\u001B[38;5;124m'\u001B[39m, path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, unzip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 12\u001B[0m     \u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_download_files\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrounakbanik/the-movies-dataset\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munzip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset ZIP already downloaded.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1664\u001B[0m, in \u001B[0;36mKaggleApi.dataset_download_files\u001B[1;34m(self, dataset, path, force, quiet, unzip, licenses)\u001B[0m\n\u001B[0;32m   1662\u001B[0m request\u001B[38;5;241m.\u001B[39mdataset_slug \u001B[38;5;241m=\u001B[39m dataset_slug\n\u001B[0;32m   1663\u001B[0m request\u001B[38;5;241m.\u001B[39mdataset_version_number \u001B[38;5;241m=\u001B[39m dataset_version_number\n\u001B[1;32m-> 1664\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mkaggle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_api_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1666\u001B[0m outfile \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(effective_path, dataset_slug \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.zip\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1667\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_needed(response, outfile, quiet):\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\kagglesdk\\datasets\\services\\dataset_api_service.py:80\u001B[0m, in \u001B[0;36mDatasetApiClient.download_dataset\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m   request \u001B[38;5;241m=\u001B[39m ApiDownloadDatasetRequest()\n\u001B[1;32m---> 80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatasets.DatasetApiService\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mApiDownloadDataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mHttpRedirect\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\kagglesdk\\kaggle_http_client.py:124\u001B[0m, in \u001B[0;36mKaggleHttpClient.call\u001B[1;34m(self, service_name, request_name, request, response_type)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# Merge environment settings into session\u001B[39;00m\n\u001B[0;32m    123\u001B[0m settings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mmerge_environment_settings(http_request\u001B[38;5;241m.\u001B[39murl, {}, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 124\u001B[0m http_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39msend(http_request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msettings)\n\u001B[0;32m    126\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_response(response_type, http_response)\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\sessions.py:724\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_redirects:\n\u001B[0;32m    722\u001B[0m     \u001B[38;5;66;03m# Redirect resolving generator.\u001B[39;00m\n\u001B[0;32m    723\u001B[0m     gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_redirects(r, request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 724\u001B[0m     history \u001B[38;5;241m=\u001B[39m [resp \u001B[38;5;28;01mfor\u001B[39;00m resp \u001B[38;5;129;01min\u001B[39;00m gen]\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    726\u001B[0m     history \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\sessions.py:724\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_redirects:\n\u001B[0;32m    722\u001B[0m     \u001B[38;5;66;03m# Redirect resolving generator.\u001B[39;00m\n\u001B[0;32m    723\u001B[0m     gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_redirects(r, request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 724\u001B[0m     history \u001B[38;5;241m=\u001B[39m [resp \u001B[38;5;28;01mfor\u001B[39;00m resp \u001B[38;5;129;01min\u001B[39;00m gen]\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    726\u001B[0m     history \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\sessions.py:265\u001B[0m, in \u001B[0;36mSessionRedirectMixin.resolve_redirects\u001B[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001B[0m\n\u001B[0;32m    263\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m req\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 265\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(\n\u001B[0;32m    266\u001B[0m         req,\n\u001B[0;32m    267\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    268\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    269\u001B[0m         verify\u001B[38;5;241m=\u001B[39mverify,\n\u001B[0;32m    270\u001B[0m         cert\u001B[38;5;241m=\u001B[39mcert,\n\u001B[0;32m    271\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[0;32m    272\u001B[0m         allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    273\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39madapter_kwargs,\n\u001B[0;32m    274\u001B[0m     )\n\u001B[0;32m    276\u001B[0m     extract_cookies_to_jar(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcookies, prepared_request, resp\u001B[38;5;241m.\u001B[39mraw)\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;66;03m# extract redirect url, if any, for the next loop\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\sessions.py:746\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    743\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[1;32m--> 746\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\models.py:902\u001B[0m, in \u001B[0;36mResponse.content\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    900\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    901\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 902\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    905\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\requests\\models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\urllib3\\response.py:1066\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m   1064\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1065\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1066\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1068\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[0;32m   1069\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\urllib3\\response.py:955\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[0;32m    952\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[0;32m    953\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[1;32m--> 955\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    957\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[0;32m    959\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\urllib3\\response.py:879\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    876\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    878\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[1;32m--> 879\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    887\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[0;32m    889\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AI\\project ai\\Recommender-System\\.venv\\lib\\site-packages\\urllib3\\response.py:862\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1()\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[1;32m--> 862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[1;32m--> 466\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting relevant files\n",
    "we will only use ratings and movies, they contain everything we need in order to make a collaborative filtering recommender"
   ],
   "id": "cf39a18ab86903d8"
  },
  {
   "cell_type": "code",
   "id": "74c72c26997cc600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:11:04.871411Z",
     "start_time": "2025-05-25T15:11:04.839784Z"
    }
   },
   "source": [
    "# Create data folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === STEP 1: Extract all necessary files (only if missing) ===\n",
    "needed_files = ['ratings.dat', 'movies.dat']\n",
    "existing_files = os.listdir(output_folder)\n",
    "\n",
    "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "    for file in needed_files:\n",
    "        if file not in existing_files:\n",
    "            print(f\"Extracting {file}...\")\n",
    "            zip_ref.extract(file, path=output_folder)\n",
    "        else:\n",
    "            print(f\"{file} already extracted.\")\n",
    "\n",
    "print(\"Extraction complete.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings.dat already extracted.\n",
      "Extracting movies.dat...\n",
      "Extraction complete.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading datasets",
   "id": "c903560c22f315b8"
  },
  {
   "cell_type": "code",
   "id": "314407a2338f259e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:05.959930Z",
     "start_time": "2025-05-25T15:35:02.117771Z"
    }
   },
   "source": [
    "# === STEP 2: Load datasets ===\n",
    "print(\"Loading datasets...\")\n",
    "movies = pd.read_csv(os.path.join(output_folder, 'movies_metadata.csv'), low_memory=False)\n",
    "ratings_path = os.path.join(output_folder, 'ratings.dat')\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep='::',\n",
    "    engine='python',\n",
    "    header=None,\n",
    "    names=['userId', 'movieId', 'rating', 'timestamp']\n",
    ")\n",
    "\n",
    "movies1m_path = os.path.join(output_folder, 'movies.dat')\n",
    "movies1m = pd.read_csv(\n",
    "    movies1m_path,\n",
    "    sep='::',\n",
    "    engine='python',\n",
    "    header=None,\n",
    "    names=['movieId', 'title', 'genre'],\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "print(\"Datasets loaded.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "a052862b82802fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:09.497043Z",
     "start_time": "2025-05-25T15:35:09.465364Z"
    }
   },
   "source": [
    "# Inspect structure of all datasets\n",
    "print(\"movies_metadata.csv columns:\\n\", movies.columns)\n",
    "print(\"ratings.csv columns:\\n\", ratings.columns)\n",
    "print(\"movies1m.csv columns:\\n\", movies1m.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_metadata.csv columns:\n",
      " Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
      "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
      "       'popularity', 'poster_path', 'production_companies',\n",
      "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
      "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n",
      "ratings.csv columns:\n",
      " Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n",
      "movies1m.csv columns:\n",
      " Index(['movieId', 'title', 'genre'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "59d66d7a635919b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:11.521331Z",
     "start_time": "2025-05-25T15:35:11.473630Z"
    }
   },
   "source": [
    "# === STEP 3: Make copies to keep originals intact ===\n",
    "movies_clean = movies.copy()\n",
    "ratings_clean = ratings.copy()\n",
    "movies1m_clean = movies1m.copy()\n",
    "movies1m_clean.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   movieId                               title                         genre\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "c1b6600924974de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:16.742463Z",
     "start_time": "2025-05-25T15:35:16.726694Z"
    }
   },
   "source": [
    "# === STEP 4: Define helper functions ===\n",
    "def parse_json_column(json_str):\n",
    "    \"\"\"Parse JSON-like string to list of names.\"\"\"\n",
    "    try:\n",
    "        items = ast.literal_eval(json_str)\n",
    "        return [item['name'] for item in items]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def safe_int_conversion(val):\n",
    "    \"\"\"Safely convert to int, return None if fails.\"\"\"\n",
    "    try:\n",
    "        return int(val)\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cleaning the data\n",
    "1. Filling NaN with 0s\n",
    "2. Removing rows with missing info\n",
    "2. Removing duplicates\n",
    "3. Removing irrelevant columns like timestamp\n",
    "4. Ensuring all columns are of the right type"
   ],
   "id": "c19021d014088327"
  },
  {
   "cell_type": "code",
   "id": "8e83d628dd36897f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:19.435224Z",
     "start_time": "2025-05-25T15:35:18.480786Z"
    }
   },
   "source": [
    "# === STEP 5: Clean movies metadata ===\n",
    "print(\"Cleaning movies metadata...\")\n",
    "# Convert budget and revenue to numeric, fill NaN with 0\n",
    "movies_clean['budget'] = pd.to_numeric(movies_clean['budget'], errors='coerce').fillna(0)\n",
    "movies_clean['revenue'] = pd.to_numeric(movies_clean['revenue'], errors='coerce').fillna(0)\n",
    "\n",
    "# Drop rows missing critical info\n",
    "movies_clean = movies_clean.dropna(subset=['title', 'id'])\n",
    "\n",
    "# Convert 'id' to numeric and drop invalid rows\n",
    "movies_clean['id'] = pd.to_numeric(movies_clean['id'], errors='coerce')\n",
    "movies_clean = movies_clean.dropna(subset=['id'])\n",
    "movies_clean['id'] = movies_clean['id'].astype(int)\n",
    "\n",
    "# Parse genres column (JSON string) into list of genre names\n",
    "movies_clean['genres'] = movies_clean['genres'].apply(parse_json_column)\n",
    "# Replace empty genres lists with ['Unknown']\n",
    "movies_clean['genres'] = movies_clean['genres'].apply(lambda x: x if x else ['Unknown'])\n",
    "print(\"Movies metadata cleaned.\\n\")\n",
    "\n",
    "# === STEP 6: Clean ratings data ===\n",
    "print(\"Cleaning ratings data...\")\n",
    "# Drop duplicates (same userId, movieId)\n",
    "ratings_clean = ratings_clean.drop_duplicates(subset=['userId', 'movieId'])\n",
    "# Drop rows with missing essential columns\n",
    "ratings_clean = ratings_clean.dropna(subset=['userId', 'movieId', 'rating'])\n",
    "# Convert types properly\n",
    "ratings_clean['userId'] = ratings_clean['userId'].astype(int)\n",
    "ratings_clean['movieId'] = ratings_clean['movieId'].astype(int)\n",
    "ratings_clean['rating'] = ratings_clean['rating'].astype(float)\n",
    "ratings_clean = ratings_clean.drop('timestamp', axis=1)\n",
    "\n",
    "\n",
    "print(\"Ratings data cleaned.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "movies1m_clean['movieId'] = movies1m_clean['movieId'].astype(int)\n",
    "movies1m_clean = movies1m_clean.drop('genre', axis=1)\n",
    "# this uses pandas' \"string\" dtype, which preserves NA\n",
    "movies1m_clean['title'] = movies1m_clean[\"title\"].replace(r\"\\s*\\(\\d{4}\\)$\", \"\", regex=True)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning movies metadata...\n",
      "Movies metadata cleaned.\n",
      "\n",
      "Cleaning ratings data...\n",
      "Ratings data cleaned.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "ca894aa0d612f851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:25.441278Z",
     "start_time": "2025-05-25T15:35:25.425595Z"
    }
   },
   "source": [
    "# === STEP 10: Summary info ===\n",
    "print(f\"Movies dataset shape: {movies_clean.shape}\")\n",
    "print(f\"Ratings dataset shape: {ratings_clean.shape}\")\n",
    "print(f\"Movies dataset shape: {movies1m_clean.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies dataset shape: (45460, 24)\n",
      "Ratings dataset shape: (1000209, 3)\n",
      "Movies dataset shape: (3883, 2)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:35:35.000717Z",
     "start_time": "2025-05-25T15:35:34.984610Z"
    }
   },
   "cell_type": "code",
   "source": "movies1m_clean.head()",
   "id": "137ddad86e4bb134",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   movieId                        title\n",
       "0        1                    Toy Story\n",
       "1        2                      Jumanji\n",
       "2        3             Grumpier Old Men\n",
       "3        4            Waiting to Exhale\n",
       "4        5  Father of the Bride Part II"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the files",
   "id": "f7516d33c82b4c86"
  },
  {
   "cell_type": "code",
   "id": "80c0583baf3f9d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:36:15.635520Z",
     "start_time": "2025-05-25T15:36:13.999635Z"
    }
   },
   "source": [
    "# === STEP 11: Save cleaned data to CSV for reuse ===\n",
    "movies_clean.to_csv(os.path.join(output_folder, 'movies_full_clean.csv'), index=False)\n",
    "ratings_clean.to_csv(os.path.join(output_folder, 'ratings_clean.csv'), index=False)\n",
    "movies1m_clean.to_csv(os.path.join(output_folder, 'movies_1m_clean.csv'), index=False)\n",
    "\n",
    "print(f\"Cleaned datasets saved in '{output_folder}' folder.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned datasets saved in 'data' folder.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extracting correct posters url for frontend\n",
    "We will use the TMDb API to get the correct poster URLs for each movie.\n",
    "\n",
    "heavily recommend using kaggle or colab for their GPUs\n",
    "\n",
    "\n"
   ],
   "id": "850045ddb5ee1449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T06:19:15.327016Z",
     "start_time": "2025-05-24T06:19:14.752120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your movie CSV\n",
    "df = pd.read_csv(os.path.join(output_folder, 'movies_full_clean.csv'))\n",
    "df = df[df['id'].notnull()].head(5000)\n",
    "df['id'] = df['id'].astype(int)\n",
    "df = df.drop(['adult', 'belongs_to_collection','budget','genres','homepage', 'imdb_id',\t'original_language', 'original_title',\t'overview',\t'popularity', 'production_companies',\t'production_countries',\t'release_date',\t'revenue',\t'runtime',\t'spoken_languages',\t'status',\t'tagline',\t'video', 'vote_average',\t'vote_count'], axis=1)\n",
    "\n",
    "\n",
    "df[\"movieId\"] = df.index + 1\n",
    "\n",
    "api_key = \"ddcd46c520d289bef23dc0f9a303a79c\"\n",
    "base_url = \"https://api.themoviedb.org/3/movie/{}?api_key={}\"\n",
    "poster_base_url = \"https://image.tmdb.org/t/p/w500\"\n",
    "\n",
    "# If partial file exists, resume\n",
    "output_file = os.path.join(output_folder, 'movies_with_tmdb_data.csv')\n",
    "if os.path.exists(output_file):\n",
    "    df_saved = pd.read_csv(output_file)\n",
    "\n",
    "    # Make sure columns exist before update\n",
    "    if 'poster_url' not in df.columns:\n",
    "        df[\"poster_url\"] = \"\"\n",
    "    if 'fetched_title' not in df.columns:\n",
    "        df[\"fetched_title\"] = \"\"\n",
    "\n",
    "    start_index = len(df_saved)\n",
    "    df.update(df_saved)\n",
    "    print(f\"Resuming from index {start_index}...\")\n",
    "else:\n",
    "    df[\"poster_url\"] = \"\"\n",
    "    df[\"fetched_title\"] = \"\"\n",
    "    start_index = 0\n",
    "\n",
    "# Loop through TMDb IDs\n",
    "for i in range(start_index, len(df)):\n",
    "    row = df.iloc[i]\n",
    "    tmdb_id = row[\"id\"]\n",
    "    try:\n",
    "        url = base_url.format(int(tmdb_id), api_key)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            df.at[i, \"poster_url\"] = (\n",
    "                poster_base_url + data.get(\"poster_path\", \"\") if data.get(\"poster_path\") else \"\"\n",
    "            )\n",
    "            df.at[i, \"fetched_title\"] = data.get(\"title\", \"\")\n",
    "        else:\n",
    "            print(f\"Error for ID {tmdb_id}: {response.status_code}\")\n",
    "        if i % 200 == 0:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Checkpoint saved at row {i}\")\n",
    "        time.sleep(0.25)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {tmdb_id}: {e}\")\n",
    "\n",
    "df = df.rename(columns={\"id\": \"tmdbId\"})\n",
    "# Final save\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"Done. Enriched metadata saved to:\", output_file)\n"
   ],
   "id": "5d46eaace505c92b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from index 5000...\n",
      "Done. Enriched metadata saved to: data\\movies_with_tmdb_data.csv\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We were working previously on the dataset with 20 million ratings, but we switched to the data with 1m dataset.",
   "id": "728e06dcf3d173cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:49:15.039322Z",
     "start_time": "2025-05-25T15:49:14.991172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Load the two files ---\n",
    "movies   = pd.read_csv(os.path.join(output_folder, \"movies_with_tmdb_data.csv\"))\n",
    "movies1m = pd.read_csv(\"data/movies_1m_clean.csv\")\n",
    "\n",
    "\n",
    "output_file = os.path.join(output_folder, 'movies1M_with_tmdb_data.csv')\n",
    "\n",
    "# --- 2. Normalise titles so they match reliably ---\n",
    "movies[\"title_clean\"]   = movies[\"title\"].str.strip().str.lower()\n",
    "movies1m[\"title_clean\"] = movies1m[\"title\"].str.strip().str.lower()\n",
    "\n",
    "# --- 3. Bring poster_url across with a left join ---\n",
    "movies1m = movies1m.merge(\n",
    "    movies[[\"title_clean\", \"poster_url\"]],\n",
    "    on=\"title_clean\",\n",
    "    how=\"left\"            # keep every row in movies1m\n",
    ")\n",
    "\n",
    "movies1m = movies1m.dropna(subset=['poster_url'])\n",
    "\n",
    "# --- 4. Drop the helper column (optional) ---\n",
    "movies1m = movies1m.drop(columns=\"title_clean\")\n",
    "\n",
    "movies1m.rename(columns={'title': 'fetched_title'}, inplace=True)\n",
    "\n",
    "\n",
    "movies1m.to_csv(output_file, index=False)\n"
   ],
   "id": "4811e671281bba52",
   "outputs": [],
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
